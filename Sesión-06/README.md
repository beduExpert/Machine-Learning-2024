 [**Inicio**](../README.md) ★ /  `Sesi贸n 01`

<div align="center">
    <img src="../BEDU.JPG" alt="Sesion_01">
</div>

##  Objetivos

锔 El objetivo de esta sesi贸n es que adquieras una comprensi贸n profunda sobre el funcionamiento de las Redes Neuronales Recurrentes (RNNs) y sus variantes, enfoc谩ndote en su capacidad para procesar secuencias de datos y capturar dependencias temporales. A trav茅s del estudio de aplicaciones como el Procesamiento de Lenguaje Natural (NLP) y el An谩lisis de Sentimientos, desarrollar谩s las habilidades necesarias para implementar RNNs en problemas reales. Adem谩s, conocer谩s las limitaciones de las RNN tradicionales y c贸mo arquitecturas avanzadas como LSTM y GRU permiten resolver dichos problemas, aplic谩ndolas en tareas de predicci贸n de secuencias y procesamiento de lenguaje.

---

 Material del prework:
En el prework podr谩s encontrar la parte te贸rica que utilizaremos para realizar los ejercicios de esta sesi贸n. 
ヂVamos a comenzar!

---

##  Temas de la sesi贸n...


###  Procesamiento de lenguaje natural

Los lenguajes naturales (tambi茅n conocidos como lenguajes ordinarios) son aquellos que son hablados o escritos por las personas para comunicaciones de prop贸sito general. El lenguaje es un sistema, un conjunto de reglas o s铆mbolos, que, combinados, conllevan informaci贸n. El Procesamiento de Lenguaje Natural (NLP, por sus siglas en ingl茅s) es un campo de la inteligencia artificial que se centra en la interacci贸n entre los computadores y el lenguaje humano. Su objetivo es hacer que las m谩quinas sean capaces de entender, interpretar y generar lenguaje de manera similar a como lo hacen los humanos.

---

###  驴Qu茅 es una RNN?

Las Redes Neuronales Recurrentes (RNN) son un tipo de red neuronal adaptada para trabajar con datos en series de tiempo. Este tipo de redes se entrenan para convertir una secuencia de datos en otra secuencia de datos, por ejemplo, la traducci贸n de textos. Las RNNs se distinguen por su capacidad para manejar secuencias de datos mediante la incorporaci贸n de ciclos en sus conexiones. Estos ciclos permiten que la informaci贸n fluya de una etapa temporal a la siguiente, lo que proporciona una "memoria" del contexto anterior.

---

###  Long short-term memory (LSTM)

Las Long Short-Term Memory (LSTM) son una arquitectura avanzada dentro de las RNNs, espec铆ficamente dise帽ada para abordar las limitaciones de las RNN tradicionales. Las RNN convencionales presentan dificultades cuando intentan aprender de secuencias de datos largas debido a problemas como el desvanecimiento y la explosi贸n del gradiente. Estos problemas surgen cuando las redes intentan retropropagar los gradientes a lo largo de muchas capas o pasos temporales, lo que provoca que los gradientes se vuelvan extremadamente peque帽os (desvanecimiento) o crezcan exponencialmente (explosi贸n). Como resultado, las RNN tienen dificultades para capturar dependencias a largo plazo, lo que limita su capacidad para modelar secuencias complejas y prolongadas.

---

###  Gated recurrent unit (GRU)

Las Gated Recurrent Units (GRU), como las LSTM, son una evoluci贸n de las redes RNN convencionales, dise帽adas para mejorar su capacidad en la captura de dependencias a largo plazo dentro de secuencias de datos. Las LSTM como las GRU, fueron dise帽adas para resolver los mismos problemas de las RNN tradicionales, sin embargo, las LSTM  lo hacen con una estructura m谩s compleja que incluye varias puertas (entrada, olvido y salida) y una c茅lula de memoria separada que controla el flujo de informaci贸n. Las GRU, por otro lado, simplifican esta arquitectura. Aunque tambi茅n utilizan un mecanismo de puertas, reducen el n煤mero de estas a dos: la puerta de actualizaci贸n y la puerta de reinicio, eliminando la c茅lula de memoria separada. 


---

### 锔 Actividades

####  **[Actividad 01: Clasificaci贸n de Sentimientos en Tweets](/Sesi贸n-06/Actividad-01/README.md)**
####  **[Actividad 02: Clasificaci贸n de Sentimientos sobre un Dataset Customizado](/Sesi贸n-06/Actividad-02/README.md)**


---

猬锔 [**Anterior**](../Sesi贸n-05/README.md) | [**Siguiente**](../Sesi贸n-07/README.md)★
